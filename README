This work was done with Noah Goodman of the CoCo lab. The project was unsuccessful for reasons described in the writeup at presentables/writeup/ProjectWriteup-KevinLeung.pdf . Below is an abstract describing the project.

Bayesian models provide a computational level description for various phenomena in cognition by treating people as ideal observers with probability distributions over hypotheses from observed data. A common criticism of these models is that they are not biologically plausible. On the other hand, connectionist models provide an algorithmic level description using neural networks.  One way to cross this divide and deal with criticisms of Bayesian models is to implement probabilistic inference in a connectionist framework. By observing the state of an integrate-and-fire network over time, we can take samples from some distribution generated by the network dynamics. Searching over various parameterizations, we have found networks that are fully generative under varying evidence with coherent posterior distributions. These results suggest that there may be some characterization of the parameters that we can learn to efficiently perform inference and ground probabilistic inference within the capabilities of the brain.

